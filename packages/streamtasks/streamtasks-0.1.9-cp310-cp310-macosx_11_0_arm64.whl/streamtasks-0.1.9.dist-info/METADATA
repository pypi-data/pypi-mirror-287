Metadata-Version: 2.1
Name: streamtasks
Version: 0.1.9
Summary: A python task orchestrator to simplify real-time, static data pipelines.
Author: leopf
License: MIT License
        
        Copyright (c) 2024 leopf
        
        Permission is hereby granted, free of charge, to any person obtaining a copy
        of this software and associated documentation files (the "Software"), to deal
        in the Software without restriction, including without limitation the rights
        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
        copies of the Software, and to permit persons to whom the Software is
        furnished to do so, subject to the following conditions:
        
        The above copyright notice and this permission notice shall be included in all
        copies or substantial portions of the Software.
        
        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
        SOFTWARE.
        
Project-URL: Homepage, https://github.com/leopf/streamtasks
Project-URL: Documentation, https://leopf.github.io/streamtasks/
Project-URL: Repository, https://github.com/leopf/streamtasks
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: typing-extensions <5.0.0,>=4.0.0
Requires-Dist: lark <2.0.0,>=1.1.9
Requires-Dist: msgpack <2.0.0,>=1.0.0
Requires-Dist: pydantic <3.0.0,>=2.7.1
Requires-Dist: numpy <2.0.0,>=1.26.4
Requires-Dist: uvicorn[standard] <1.0.0,>=0.29.0
Provides-Extra: dev
Requires-Dist: ruff ; extra == 'dev'
Provides-Extra: inference
Requires-Dist: speechbrain <2.0.0,>=1.0.0 ; extra == 'inference'
Requires-Dist: torch <3.0.0,>=2.0.0 ; extra == 'inference'
Requires-Dist: llama-cpp-python <1.0.0,>=0.2.0 ; extra == 'inference'
Requires-Dist: transformers <5.0.0,>=4.30.0 ; extra == 'inference'
Provides-Extra: media
Requires-Dist: av <13.0.0,>=12.0.0 ; extra == 'media'
Requires-Dist: opencv-python <5.0.0,>=4.9.0.80 ; extra == 'media'
Requires-Dist: sounddevice <1.0.0,>=0.4.0 ; extra == 'media'
Requires-Dist: mss <10.0.0,>=9.0.0 ; extra == 'media'
Requires-Dist: pillow >=10.0.0 ; extra == 'media'
Provides-Extra: testing
Requires-Dist: httpx <1.0.0,>=0.27.0 ; extra == 'testing'


# streamtasks

![](docs/screenshot.png)

Read the [Documentation](https://leopf.github.io/streamtasks).

## Demos
- [llama.cpp chatbot](https://x.com/leopfff/status/1805094491100971056)
- [playing sound effects](https://x.com/leopfff/status/1805128606839931044)
- [llama.cpp + tts](https://x.com/leopfff/status/1805138123187224636)
- [video layout and video viewer](https://x.com/leopfff/status/1805205696624906694)
- [audio switching](https://x.com/leopfff/status/1805502946357940703)
- [audio mixing and scaling](https://x.com/leopfff/status/1805508727832780978)
- [publishing a livestream](https://x.com/leopfff/status/1805941200802758943)

## Overview

Streamtasks aims to simplify software integration for data pipelines.

### How it works
Streamtasks is built on an internal network that distributes messages. The network is host agnostic. It uses the same network to communicate with services running in the same process as it does to communicate with services on a remote machine.

## Getting started

### Installation
```bash
pip install streamtasks[media,inference] # see pyproject.toml for more optional packages
```

#### Hardware encoders and decoders
To use hardware encoders and decoders you must have ffmpeg installed on your system.
Verify that you system installation of ffmpeg has the hardware encoders/decoder with:
```bash
# list decoders
ffmpeg -decoders
# list encoders
ffmpeg -encoders
```
Install streamtasks without `av` binaries.
```bash
pip install streamtasks[media,inference] --no-binary av
```
If you have already installed streamtasks (and `av`), you can reinstall `av` with:
```bash
pip install av --no-binary av --ignore-installed
```

See [the pyav documentation](https://pyav.org/docs/develop/overview/installation.html) for more information.

#### llama.cpp with GPU
To install llama.cpp with GPU support you can either install streamtasks with:
```bash 
CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install streamtasks[media,inference]
```

or you can reinstall llama-cpp-python with:

```bash 
CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python --ignore-installed
```
See [the llama-cpp-python documentation](https://github.com/abetlen/llama-cpp-python) for more information.


### Running an instance
You can run an instance of the streamtasks system with `streamtasks -C` or `python -m streamtasks -C`.

The flag `-C` indicates that the core components should be started as well.

Use `streamtasks --help` for more options.

### Connecting two instances
When connecting two instances you need to have one main instance running the core components (using `-C`).

To create a connection endpoint (server), you can use the Connection Manager in the UI or you can specify a url to host a server on as a command line flag.

For example:
```bash
streamtasks -C --serve tcp://127.0.0.1:9002
```

You may specify multiple serve urls.

To connect the second system to the main system, you need to start your second system **without** the core components, specifying a connect url.

For example:
```bash
streamtasks --connect tcp://127.0.0.1:9002
```

See [connection](docs/connection.md) for more information.
