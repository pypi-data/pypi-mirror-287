Metadata-Version: 2.1
Name: runhouse
Version: 0.0.32
Summary: Runhouse: A multiplayer cloud compute and data environment
Author: Runhouse Team
License: Apache 2.0
Project-URL: Homepage, https://run.house
Project-URL: Issues, https://github.com/run-house/runhouse/issues/
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: OS Independent
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: System :: Distributed Computing
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: python-dotenv
Requires-Dist: fastapi
Requires-Dist: fsspec <=2023.5.0
Requires-Dist: pexpect
Requires-Dist: pyOpenSSL >=23.3.0
Requires-Dist: ray[default] !=2.6.0,>=2.2.0
Requires-Dist: async-timeout
Requires-Dist: rich
Requires-Dist: sentry-sdk
Requires-Dist: setuptools <70.0.0
Requires-Dist: sshfs <=2023.4.1,>=2023.1.0
Requires-Dist: typer
Requires-Dist: uvicorn
Requires-Dist: wheel
Requires-Dist: apispec
Requires-Dist: httpx
Provides-Extra: all
Requires-Dist: skypilot ==0.6.0 ; extra == 'all'
Requires-Dist: pandas ; extra == 'all'
Requires-Dist: pyarrow ; extra == 'all'
Requires-Dist: skypilot[aws] ==0.6.0 ; extra == 'all'
Requires-Dist: pycryptodome ==3.12.0 ; extra == 'all'
Requires-Dist: sshtunnel >=0.3.0 ; extra == 'all'
Requires-Dist: skypilot[azure] ==0.6.0 ; extra == 'all'
Requires-Dist: skypilot[gcp] ==0.6.0 ; extra == 'all'
Requires-Dist: gcsfs ; extra == 'all'
Requires-Dist: docker ; extra == 'all'
Requires-Dist: sagemaker-ssh-helper ; extra == 'all'
Requires-Dist: sagemaker ; extra == 'all'
Requires-Dist: paramiko >=3.2.0 ; extra == 'all'
Requires-Dist: kubernetes ; extra == 'all'
Provides-Extra: aws
Requires-Dist: skypilot[aws] ==0.6.0 ; extra == 'aws'
Requires-Dist: pycryptodome ==3.12.0 ; extra == 'aws'
Requires-Dist: sshtunnel >=0.3.0 ; extra == 'aws'
Provides-Extra: azure
Requires-Dist: skypilot[azure] ==0.6.0 ; extra == 'azure'
Provides-Extra: data
Requires-Dist: pandas ; extra == 'data'
Requires-Dist: pyarrow ; extra == 'data'
Provides-Extra: docker
Requires-Dist: docker ; extra == 'docker'
Provides-Extra: gcp
Requires-Dist: skypilot[gcp] ==0.6.0 ; extra == 'gcp'
Requires-Dist: gcsfs ; extra == 'gcp'
Provides-Extra: kubernetes
Requires-Dist: skypilot ==0.6.0 ; extra == 'kubernetes'
Requires-Dist: kubernetes ; extra == 'kubernetes'
Provides-Extra: sagemaker
Requires-Dist: skypilot ==0.6.0 ; extra == 'sagemaker'
Requires-Dist: sagemaker-ssh-helper ; extra == 'sagemaker'
Requires-Dist: sagemaker ; extra == 'sagemaker'
Requires-Dist: paramiko >=3.2.0 ; extra == 'sagemaker'
Provides-Extra: sky
Requires-Dist: skypilot ==0.6.0 ; extra == 'sky'

# ğŸƒâ€â™€ï¸RunhouseğŸ 

[![Discord](https://dcbadge.vercel.app/api/server/RnhB6589Hs?compact=true&style=flat)](https://discord.gg/RnhB6589Hs)
[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/runhouse_.svg?style=social&label=@runhouse_)](https://twitter.com/runhouse_)
[![Website](https://img.shields.io/badge/run.house-green)](https://www.run.house)
[![Docs](https://img.shields.io/badge/docs-blue)](https://www.run.house/docs)
[![Den](https://img.shields.io/badge/runhouse_den-purple)](https://www.run.house/login)

## ğŸ‘µ Welcome Home!

Runhouse gives your code the superpower of traversing remote infrastructure, so you
can iterate and debug your ML apps and workflows locally in regular Python (no DSLs, yaml, or prescriptive
dev environment) with full-scale compute and data (no sandbox). It's the fastest way to build, run,
and deploy production-quality ML apps and workflows on your own infrastructure, and perhaps the only way to
take production code and run it as-is locally (again, running on identical powerful infra) to iterate it further or
debug.

After you've sent a function or class to remote compute, Runhouse also allows you to persist, reuse, and share it as
a service, turning otherwise redundant AI activities into common modular components across your team or company.
This improves cost, velocity, and reproducibility - think 10 ML pipelines and researchers calling the same shared
preprocessing, training, evaluation, or batch inference service, rather than each allocating their own compute
resources and deploying slightly differing code. Or, imagine experimenting with a new preprocessing method in a
notebook, but you can call every other stage of your ML workflow as the production services themselves.

Highlights:
* ğŸ‘©â€ğŸ”¬ Dispatch Python functions, classes, and data to remote infra instantly, and call
them eagerly as if they were local. Logs are streamed, iteration is fast.
* ğŸ‘·â€â™€ï¸ Share Python functions or classes as robust services, including HTTPS, auth, observability,
scaling, custom domains, secrets, versioning, and more.
* ğŸ No DSL, decorators, yaml, CLI incantations, or boilerplate. Just your own regular Python.
* ğŸš€ Deploy anywhere you run Python. No special packaging or deployment process. Research and production code are
identical.
* ğŸ‘©â€ğŸ“ BYO-infra with extensive and growing support - Ray, Kubernetes, AWS, GCP, Azure, local, on-prem, and more.
When you want to shift or scale, just send your code to more powerful infra.
* ğŸ‘©â€ğŸš€ Extreme reproducibility and portability. A single succinct script can allocate the infra, set up dependencies,
and serve your app.
* ğŸ‘©â€ğŸ³ Nest applications to create complex workflows and services. Components are decoupled so you can change,
shift, or scale any component without affecting the rest of your system.

The Runhouse API is dead simple. Send your **modules** (functions and classes) into **environments** on compute
**infra**, like this:

```python
import runhouse as rh
from diffusers import StableDiffusionPipeline

def sd_generate(prompt, **inference_kwargs):
    model = StableDiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-base").to("cuda")
    return model(prompt, **inference_kwargs).images

if __name__ == "__main__":
    gpu = rh.cluster(name="rh-a10x", instance_type="A10G:1", provider="aws")
    sd_env = rh.env(reqs=["torch", "transformers", "diffusers"], name="sd_generate")

    # Deploy the function and environment (syncing over local code changes and installing dependencies)
    remote_sd_generate = rh.function(sd_generate).to(gpu, env=sd_env)

    # This call is actually an HTTP request to the app running on the remote server
    imgs = remote_sd_generate("A hot dog made out of matcha.")
    imgs[0].show()

    # You can also call it over HTTP directly, e.g. from other machines or languages
    print(remote_sd_generate.endpoint())
```

With the above simple structure you can build, call, and share:
* ğŸ› ï¸ **AI primitives**: Preprocessing, training, fine-tuning, evaluation, inference
* ğŸš€ **Higher-order services**: Multi-step inference, e2e workflows, evaluation gauntlets, HPO
* ğŸ§ª **UAT endpoints**: Instant endpoints for client teams to test and integrate
* ğŸ¦º **Best-practice utilities**: PII obfuscation, content moderation, data augmentation


## ğŸ›‹ï¸ Sharing and Versioning with Runhouse Den

You can unlock unique accessibility and sharing features with
[Runhouse Den](https://www.run.house/dashboard), a complimentary product to this repo.
Log in from anywhere to save, share, and load resources:
```shell
runhouse login
```
or from Python:
```python
import runhouse as rh
rh.login()
```

Extending the example above to share and load our app via Den:

```python
remote_sd_generate.share(["my_pal@email.com"])

# The service stub can now be reloaded from anywhere, always at yours and your collaborators' fingertips
# Notice this code doesn't need to change if you update, move, or scale the service
remote_sd_generate = rh.function("/your_username/sd_generate")
imgs = remote_sd_generate("More matcha hotdogs.")
imgs[0].show()
```

## <h2 id="supported-infra"> ğŸ—ï¸ Supported Compute Infra </h2>

Please reach out (first name at run.house) if you don't see your favorite compute here.
  - Local - **Supported**
  - Single box - **Supported**
  - Ray cluster - **Supported**
  - Kubernetes - **Supported**
  - Amazon Web Services (AWS)
    - EC2 - **Supported**
    - EKS - **Supported**
    - SageMaker - **Supported**
    - Lambda - **Alpha**
  - Google Cloud Platform (GCP)
    - GCE - **Supported**
    - GKE - **Supported**
  - Microsoft Azure
    - VMs - **Supported**
    - AKS - **Supported**
  - Lambda Labs - **Supported**
  - Modal Labs - Planned
  - Slurm - Exploratory

## ğŸ‘¨â€ğŸ« Learn More

[**ğŸ£ Getting Started**](https://www.run.house/docs/tutorials/cloud_quick_start): Installation, setup, and a quick walkthrough.

[**ğŸ“– Docs**](https://www.run.house/docs):
Detailed API references, basic API examples and walkthroughs, end-to-end tutorials, and high-level architecture overview.

[**ğŸ‘©â€ğŸ’» Blog**](https://www.run.house/blog): Deep dives into Runhouse features, use cases, and the future of AI
infra.

[**ğŸ‘¾ Discord**](https://discord.gg/RnhB6589Hs): Join our community to ask questions, share ideas, and get help.

[**ğ‘‹ Twitter**](https://twitter.com/runhouse_): Follow us for updates and announcements.

## ğŸ™‹â€â™‚ï¸ Getting Help

Message us on [Discord](https://discord.gg/RnhB6589Hs), email us (first name at run.house), or create an issue.

## ğŸ‘·â€â™€ï¸ Contributing

We welcome contributions! Please check out [contributing](CONTRIBUTING.md).
