# coding: utf-8

"""
    Versatile Data Kit Control Service API

    The Data Jobs API of Versatile Data Kit Control Service. Data Jobs allows Data Engineers to implement automated pull ingestion (E in ELT) and batch data transformation into a database (T in ELT). See also https://github.com/vmware/versatile-data-kit/wiki/Introduction The API has resource-oriented URLs, JSON-encoded responses, and uses standard HTTP response codes, authentication, and verbs. The API enables creating, deploying, managing and executing Data Jobs in the runtime environment.<br> <br> ![](https://github.com/vmware/versatile-data-kit/wiki/vdk-data-job-lifecycle-state-diagram.png) <br> The API reflects the usual Data Job Development lifecycle:<br> <li> Create a new data job (webhook to further configure the job, e.g authorize its creation, setup permissions, etc). <li> Download keytab. Develop and run the data job locally. <li> Deploy the data job in cloud runtime environment to run on a scheduled basis. <br><br> If Authentication is enabled, pass OAuth2 access token in HTTP header 'Authorization: Bearer [access-token-here]' (https://datatracker.ietf.org/doc/html/rfc6750). <br The API promotes some best practices (inspired by https://12factor.net): <li> Explicitly declare and isolate dependencies. <li> Strict separation of configurations from code. Configurations vary substantially across deploys, code does not. <li> Separation between the build, release/deploy, and run stages. <li> Data Jobs are stateless and share-nothing processes. Any data that needs to be persisted must be stored in a stateful backing service (e.g IProperties). <li> Implementation is assumed to be atomic and idempotent - should be OK for a job to fail somewhere in the middle; subsequent restart should not cause data corruption. <li> Keep development, staging, and production as similar as possible. <br><br> <b>API Evolution</b><br> In the following sections, there are some terms that have a special meaning in the context of the APIs. <br><br> <li> <i>Stable</i> - The implementation of the API has been battle-tested (has been in production for some time). The API is a subject to semantic versioning model and will follow deprecation policy. <li> <i>Experimental</i> - May disappear without notice and is not a subject to semantic versioning. Implementation of the API is not considered stable nor well tested. Generally this is given to clients to experiment within testing environment. Must not be used in production. <li> <i>Deprecated</i> - API is expected to be removed within next one or two major version upgrade. The deprecation notice/comment will say when the API will be removed and what alternatives should be used instead.  # noqa: E501

    The version of the OpenAPI document: 1.0
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from datetime import datetime
from typing import Optional
from pydantic import BaseModel, Field, StrictStr, validator
from taurus_datajob_api.models.data_job_deployment import DataJobDeployment

class DataJobExecution(BaseModel):
    """
    Executions of a Data Job
    """
    id: Optional[StrictStr] = Field(None, description="Data Job Execution ID")
    job_name: Optional[StrictStr] = Field(None, description="Data Job name")
    status: Optional[StrictStr] = Field(None, description="The current status")
    type: Optional[StrictStr] = Field(None, description="Execution type - manual or scheduled")
    start_time: Optional[datetime] = Field(None, description="Start of execution")
    end_time: Optional[datetime] = Field(None, description="Start of execution")
    started_by: Optional[StrictStr] = Field(None, description="User or service that started the execution (e.g manual/auserov@example.mail.com or scheduled/runtime)")
    logs_url: Optional[StrictStr] = Field(None, description="URL link to persisted logs in central location. Logs generally should be available for longer time. The link is available only if operators have configured it during installation of Control Service. During install operators can configure logs to be persisted to log aggregator service whose link can be exposed here. ")
    message: Optional[StrictStr] = Field(None, description="Message (usually error) during execution")
    op_id: Optional[StrictStr] = Field(None, description="Operation id used for tracing calls between different services")
    deployment: Optional[DataJobDeployment] = None
    __properties = ["id", "job_name", "status", "type", "start_time", "end_time", "started_by", "logs_url", "message", "op_id", "deployment"]

    @validator('status')
    def status_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in ('submitted', 'running', 'succeeded', 'cancelled', 'skipped', 'user_error', 'platform_error'):
            raise ValueError("must be one of enum values ('submitted', 'running', 'succeeded', 'cancelled', 'skipped', 'user_error', 'platform_error')")
        return value

    @validator('type')
    def type_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in ('manual', 'scheduled'):
            raise ValueError("must be one of enum values ('manual', 'scheduled')")
        return value

    class Config:
        """Pydantic configuration"""
        allow_population_by_field_name = True
        validate_assignment = True

    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.dict(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> DataJobExecution:
        """Create an instance of DataJobExecution from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self):
        """Returns the dictionary representation of the model using alias"""
        _dict = self.dict(by_alias=True,
                          exclude={
                          },
                          exclude_none=True)
        # override the default output from pydantic by calling `to_dict()` of deployment
        if self.deployment:
            _dict['deployment'] = self.deployment.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: dict) -> DataJobExecution:
        """Create an instance of DataJobExecution from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return DataJobExecution.parse_obj(obj)

        _obj = DataJobExecution.parse_obj({
            "id": obj.get("id"),
            "job_name": obj.get("job_name"),
            "status": obj.get("status"),
            "type": obj.get("type"),
            "start_time": obj.get("start_time"),
            "end_time": obj.get("end_time"),
            "started_by": obj.get("started_by"),
            "logs_url": obj.get("logs_url"),
            "message": obj.get("message"),
            "op_id": obj.get("op_id"),
            "deployment": DataJobDeployment.from_dict(obj.get("deployment")) if obj.get("deployment") is not None else None
        })
        return _obj

