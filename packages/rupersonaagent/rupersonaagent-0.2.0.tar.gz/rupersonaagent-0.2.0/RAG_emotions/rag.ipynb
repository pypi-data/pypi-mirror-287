{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stc/disk/vologina/my_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from peft import AutoPeftModelForCausalLM, PeftModel, PeftConfig\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, AutoModel\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import getpass\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_api_key = getpass.getpass(\"Enter your HF Inference API Key:\\n\\n\")\n",
    "#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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/stc/disk/vologina/df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  ДАТАСЕТ В ФОРМАТ ДОКУМЕНТА\n",
    "class DataFrameLoader:\n",
    "    def __init__(self, df, page_content_column):\n",
    "        self.df = df\n",
    "        self.page_content_column = page_content_column\n",
    "\n",
    "    def load(self):\n",
    "        return [Document(text) for text in self.df[self.page_content_column].tolist()]\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, page_content):\n",
    "        self.page_content = page_content\n",
    "\n",
    "loader = DataFrameLoader(df, page_content_column='text')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделяем документы на части\n",
    "class RecursiveCharacterTextSplitter:\n",
    "    def __init__(self, chunk_size, chunk_overlap):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "    def split_documents(self, documents):\n",
    "        texts = []\n",
    "        for document in documents:\n",
    "            text = document.page_content\n",
    "            for i in range(0, len(text), self.chunk_size):\n",
    "                end = i + self.chunk_size\n",
    "                if end >= len(text):\n",
    "                    texts.append(text[i:])\n",
    "                else:\n",
    "                    texts.append(text[i:end + self.chunk_overlap])\n",
    "        return texts\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataFrameLoader(df, page_content_column='text')\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/home/stc/disk/vologina/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('/home/stc/disk/vologina/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_bert_cls(texts, model, tokenizer):\n",
    "    t = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13392 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13392/13392 [01:47<00:00, 124.98it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings_list = []\n",
    "for document in tqdm(documents):\n",
    "    text = document.page_content\n",
    "    embeddings = embed_bert_cls(text, model, tokenizer)\n",
    "    embeddings_list.append(embeddings)\n",
    "    \n",
    "all_embeddings = np.vstack(embeddings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = all_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(db)\n",
    "index.add(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaissRetrieverCosine:\n",
    "  def __init__(self, embeddings):\n",
    "        self.embeddings = embeddings\n",
    "        # Используем IndexFlatIP для косинусной близости\n",
    "        self.index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "        self.index.add(embeddings)\n",
    "  def retrieve(self, query_embedding, k=5):\n",
    "        query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "        _, indices = self.index.search(query_embedding, k)\n",
    "        return indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_retriever_cosine = FaissRetrieverCosine(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:   периода\n",
      "2:  подумой\n",
      "3:   А что по прогнозу погоды\n",
      "4:  правда\n",
      "5:  правда \n"
     ]
    }
   ],
   "source": [
    "query_embedding = embed_bert_cls(\"погода\", model, tokenizer)\n",
    "\n",
    "top_k_indices = faiss_retriever_cosine.retrieve(query_embedding, k=5)\n",
    "\n",
    "for i, idx in enumerate(top_k_indices):\n",
    "    print(f\"{i + 1}: {texts[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarities: [[0.93256277 0.9159932  0.91475314 0.9094141  0.9094141 ]]\n"
     ]
    }
   ],
   "source": [
    "retrieved_embeddings = all_embeddings[top_k_indices]\n",
    "\n",
    "cosine_similarities = cosine_similarity(query_embedding.reshape(1, -1), retrieved_embeddings)\n",
    "print(\"Cosine Similarities:\", cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [18:46<00:00, 187.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"max_new_tokens\": 1536,\n",
      "  \"no_repeat_ngram_size\": 15,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"repetition_penalty\": 1.15,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 30,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cache_dir = '/mnt/storage/cache_huggingface'\n",
    "config = PeftConfig.from_pretrained('/mnt/storage/cache_huggingface/models--evilfreelancer--ruGPT-3.5-13B-lora/snapshots/c316657abba32a553840064d6f03d08e64bda201')\n",
    "model_gen = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    load_in_8bit=True,\n",
    "    cache_dir = cache_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True\n",
    ")\n",
    "model_gen = PeftModel.from_pretrained(\n",
    "    model_gen,\n",
    "    '/home/stc/disk/vologina/ruGPT-3.5-13B-lora',\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model_gen.eval()\n",
    "tokenizer_gen = AutoTokenizer.from_pretrained('/home/stc/disk/vologina/ruGPT-3.5-13B-lora')\n",
    "\n",
    "generation_config = GenerationConfig.from_pretrained('/home/stc/disk/vologina/ruGPT-3.5-13B-lora')\n",
    "print(generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"Ты — русскоязычный эмоциональный ассистент. Ты эмоционально разговариваешь с людьми и помогаешь им.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Класс для ведения беседы\n",
    "class Conversation:\n",
    "    def __init__(self, message_template=\"<s>{role}\\n{content}</s>\\n\", system_prompt=\"Ты — русскоязычный эмоциональный ассистент. Ты эмоционально разговариваешь с людьми и помогаешь им.\", start_token_id=2, bot_token_id=46787):\n",
    "        self.message_template = message_template\n",
    "        self.start_token_id = start_token_id\n",
    "        self.bot_token_id = bot_token_id\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "    def add_user_message(self, message):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    def add_bot_message(self, message):\n",
    "        self.messages.append({\"role\": \"bot\", \"content\": message})\n",
    "\n",
    "    def get_prompt(self, tokenizer):\n",
    "        final_text = \"\"\n",
    "        for message in self.messages:\n",
    "            message_text = self.message_template.format(**message)\n",
    "            final_text += message_text\n",
    "        final_text += tokenizer.decode([self.start_token_id, self.bot_token_id])\n",
    "        return final_text.strip()\n",
    "\n",
    "\n",
    "# Функция для генерации ответа\n",
    "def generate(model, tokenizer, prompt, generation_config):\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "    output_ids = model.generate(**data, generation_config=generation_config)[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    return output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для выполнения запроса RAG\n",
    "def rag_query(user_query):\n",
    "    query_embedding = embed_bert_cls(user_query, model, tokenizer)\n",
    "    top_k_indices = faiss_retriever_cosine.retrieve(query_embedding, k=5)\n",
    "    \n",
    "    relevant_texts = [texts[idx] for idx in top_k_indices]\n",
    "    context = \"\\n\".join(relevant_texts)\n",
    "    \n",
    "    conversation = Conversation()\n",
    "    conversation.add_user_message(f\"Контекст: {context}\\nВопрос: {user_query}\")\n",
    "    \n",
    "    prompt = conversation.get_prompt(tokenizer_gen)\n",
    "    output = generate(model_gen, tokenizer_gen, prompt, generation_config)\n",
    "    \n",
    "    conversation.add_bot_message(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ruGPT-3.5: Ответ: Прогнозы погоды, как правило, включают данные о температуре воздуха на определенной высоте (обычно это 10-20 метров), скорости ветра и влажности. Они могут быть очень точными, но все же не всегда точны. В целом, погода может меняться каждый день или даже несколько раз за день.\n",
      "ruGPT-3.5: Год 2017-й, месяц январь.\n",
      "ruGPT-3.5: Конечно, это точный ответ!\n"
     ]
    }
   ],
   "source": [
    "# Интерфейс для взаимодействия\n",
    "while True:\n",
    "    user_input = input(\"Вы: \")\n",
    "\n",
    "    if user_input.strip() == \"/заново\":\n",
    "        conversation = Conversation()\n",
    "        print(\"История сброшена!\")\n",
    "        continue\n",
    "\n",
    "    if user_input.strip() == \"\":\n",
    "        continue\n",
    "\n",
    "    if user_input.strip().lower() == \"/стоп\":\n",
    "        break\n",
    "\n",
    "    response = rag_query(user_input)\n",
    "    print(\"ruGPT-3.5:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
