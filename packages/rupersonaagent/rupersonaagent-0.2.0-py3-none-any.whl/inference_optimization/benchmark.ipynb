{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "t5_cpu = T5ForConditionalGeneration.from_pretrained(\"cointegrated/rut5-base-multitask\", resume_download=True).eval()\n",
    "t5_cuda = t5_cpu.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.benchmark as benchmark\n",
    "import os\n",
    "\n",
    "num_threads = 16\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(num_threads)\n",
    "os.environ[\"MKL_NUM_THREADS\"] = str(num_threads)\n",
    "\n",
    "torch.set_num_threads(num_threads)\n",
    "\n",
    "input_cpu = torch.randint(high=30000, size=(1, 32), dtype=torch.int64)\n",
    "input_cuda = torch.randint(high=30000, size=(1, 32), dtype=torch.int64).cuda()\n",
    "\n",
    "t5_cpu.generate(input_cpu, do_sample=True, num_beams=4, max_new_tokens=20)\n",
    "t5_cuda.generate(input_cuda, do_sample=True, num_beams=4, max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
    "import onnxruntime as ort\n",
    "\n",
    "sess_options = ort.SessionOptions()\n",
    "\n",
    "sess_options.intra_op_num_threads = num_threads\n",
    "sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL\n",
    "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "t5_ort_cpu = ORTModelForSeq2SeqLM.from_pretrained(\"cointegrated/rut5-base-multitask\",\n",
    "                                                   export=True,\n",
    "                                                   provider=\"CPUExecutionProvider\",\n",
    "                                                  session_options=sess_options)\n",
    "\n",
    "t5_ort_cuda = ORTModelForSeq2SeqLM.from_pretrained(\"cointegrated/rut5-base-multitask\",\n",
    "                                                   from_transformers=True,\n",
    "                                                   provider=\"CUDAExecutionProvider\")\n",
    "t5_ort_cpu.generate(input_cpu, do_sample=True, num_beams=4, max_new_tokens=20)\n",
    "t5_ort_cuda.generate(input_cuda, do_sample=True, num_beams=4, max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eet.transformers.modeling_t5 import EETT5ForConditionalGeneration\n",
    "\n",
    "t5_eet = EETT5ForConditionalGeneration.from_pretrained(\"cointegrated/rut5-base-multitask\", 1)\n",
    "t5_eet.generate(input_cuda, do_sample=True, num_beams=4, max_new_tokens=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightseq.inference as lsi\n",
    "\n",
    "t5_ls = lsi.T5(\"lightseq_t5_base.hdf5\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_ls.infer(input_cpu.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
    "\n",
    "t5_ort_trt = ORTModelForSeq2SeqLM.from_pretrained(\"cointegrated/rut5-base-multitask\",\n",
    "                                      from_transformers=True,\n",
    "                                      provider=\"TensorrtExecutionProvider\")\n",
    "t5_ort_trt.generate(input_cuda, do_sample=True, num_beams=4, max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
    "import onnxruntime as ort\n",
    "\n",
    "options = ort.SessionOptions()\n",
    "options.intra_op_num_threads = num_threads\n",
    "options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL  # https://onnxruntime.ai/docs/execution-providers/OpenVINO-ExecutionProvider.html#other-configuration-settings\n",
    "\n",
    "t5_ort_openvino = ORTModelForSeq2SeqLM.from_pretrained(\"cointegrated/rut5-base-multitask\",\n",
    "                                                       export=True,\n",
    "                                                       provider=\"OpenVINOExecutionProvider\",\n",
    "                                                       session_options=options,\n",
    "                                                       provider_options={\"num_of_threads\": num_threads})\n",
    "t5_ort_openvino.generate(input_cpu, do_sample=True, num_beams=4, max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.intel.openvino import OVModelForSeq2SeqLM\n",
    "\n",
    "t5_openvino = OVModelForSeq2SeqLM.from_pretrained(\"cointegrated/rut5-base-multitask\",\n",
    "                                                   export=True, use_cache=True, compile=True, ov_config={\"INFERENCE_NUM_THREADS\": num_threads})\n",
    "t5_openvino.generate(input_cpu, do_sample=True, num_beams=2, max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CPU benchmark.\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer\n",
    "import os\n",
    "\n",
    "num_runs = 100\n",
    "\n",
    "results = {}\n",
    "for seq_len in [8, 32, 64]:\n",
    "    print(f\"Sequence length: {seq_len}\")\n",
    "    input_cpu = torch.randint(high=30000, size=(1, seq_len), dtype=torch.int64)\n",
    "    for model, description in zip([t5_cpu, t5_ort_cpu, t5_ort_openvino, t5_openvino],\n",
    "                                  [\"PyTorch (CPU)\", \"ORT (CPU)\", \"ORT (OpenVINO)\", \"Optimum OpenVINO\"]):\n",
    "        model.generate(input_cpu, do_sample=True, num_beams=4, max_new_tokens=1)  # warmup\n",
    "        start_time = default_timer()\n",
    "        for i in range(num_runs):\n",
    "            model.generate(input_cpu, do_sample=True, num_beams=4, max_new_tokens=1)\n",
    "        print(f\"{description}\\t{(default_timer() - start_time) / num_runs}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For GPU benchmark.\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer\n",
    "import os\n",
    "\n",
    "num_runs = 100\n",
    "\n",
    "results = {}\n",
    "for seq_len in [8, 32, 64]:\n",
    "    print(f\"Sequence length: {seq_len}\")\n",
    "    input_cpu = torch.randint(high=30000, size=(1, seq_len), dtype=torch.int64)\n",
    "    input_cuda = torch.randint(high=30000, size=(1, seq_len), dtype=torch.int64, device=\"cuda\")\n",
    "    for model, description in zip([t5_cuda, t5_eet, t5_ls], #t5_ort_cuda],\n",
    "                                  [\"PyTorch (CUDA)\", \"EET\", \"LightSeq\"]): #\"ORT (CUDA)\"]):\n",
    "        if description == \"LightSeq\":\n",
    "            model.infer(input_cpu.numpy())\n",
    "        else:\n",
    "            model.generate(input_cuda, do_sample=True, num_beams=4, max_new_tokens=1)  # warmup\n",
    "        torch.cuda.synchronize()\n",
    "        start_time = default_timer()\n",
    "        for i in range(num_runs):\n",
    "            if description == \"LightSeq\":\n",
    "                model.infer(input_cpu.numpy())\n",
    "            else:\n",
    "                model.generate(input_cuda, do_sample=True, num_beams=4, max_new_tokens=1)\n",
    "        torch.cuda.synchronize()\n",
    "        print(f\"{description}\\t{(default_timer() - start_time) / num_runs}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(input_cuda, do_sample=True, num_beams=4, max_new_tokens=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}