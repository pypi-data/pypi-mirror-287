---
license: mit
datasets:
  - IlyaGusev/ru_turbo_alpaca
  - IlyaGusev/ru_turbo_alpaca_evol_instruct
  - IlyaGusev/ru_turbo_saiga
  - IlyaGusev/ru_sharegpt_cleaned
  - IlyaGusev/oasst1_ru_main_branch
  - IlyaGusev/gpt_roleplay_realm
  - lksy/ru_instruct_gpt4
language:
  - ru
  - en
library_name: peft
pipeline_tag: conversational
tags:
  - Saiga
  - ruGPT-3.5
  - 13B
  - chat
  - lora
  - Peft
  - adapter
---

# ruGPT-3.5 13B LoRA: Adapter-Only Version

Welcome to the adapter-only version of ruGPT-3.5 13B LoRA. This model is built upon the foundation of [ruGPT-3.5-13B](https://huggingface.co/ai-forever/ruGPT-3.5-13B).

üìå Important: This model was trained using settings identical to [GigaSaiga](https://huggingface.co/IlyaGusev/gigasaiga_lora), but incorporates additional dataset.

üîó Training code is [here](https://github.com/EvilFreelancer/ruGPT-3.5-13B-lora).

> Note: If you prefer, you can opt to use the ruGPT-3.5 13B fp16 base model.

## Code sample

```python
import torch
from peft import PeftModel, PeftConfig
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig

MODEL_NAME = "evilfreelancer/ruGPT-3.5-13B-lora"
DEFAULT_MESSAGE_TEMPLATE = "<s>{role}\n{content}</s>\n"
DEFAULT_SYSTEM_PROMPT = "–¢—ã ‚Äî ruGPT-3.5, —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –Ω–∞ 13 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –¢—ã —Ä–∞–∑–≥–æ–≤–∞—Ä–∏–≤–∞–µ—à—å —Å –ª—é–¥—å–º–∏ –∏ –ø–æ–º–æ–≥–∞–µ—à—å –∏–º."

class Conversation:
    def __init__(
        self,
        message_template=DEFAULT_MESSAGE_TEMPLATE,
        system_prompt=DEFAULT_SYSTEM_PROMPT,
        start_token_id=2,
        bot_token_id=46787
    ):
        self.message_template = message_template
        self.start_token_id = start_token_id
        self.bot_token_id = bot_token_id
        self.messages = [{
            "role": "system",
            "content": system_prompt
        }]

    def get_start_token_id(self):
        return self.start_token_id

    def get_bot_token_id(self):
        return self.bot_token_id

    def add_user_message(self, message):
        self.messages.append({
            "role": "user",
            "content": message
        })

    def add_bot_message(self, message):
        self.messages.append({
            "role": "bot",
            "content": message
        })

    def get_prompt(self, tokenizer):
        final_text = ""
        for message in self.messages:
            message_text = self.message_template.format(**message)
            final_text += message_text
        final_text += tokenizer.decode([self.start_token_id, self.bot_token_id])
        return final_text.strip()


def generate(model, tokenizer, prompt, generation_config):
    data = tokenizer(prompt, return_tensors="pt")
    data = {k: v.to(model.device) for k, v in data.items()}
    output_ids = model.generate(
        **data,
        generation_config=generation_config
    )[0]
    output_ids = output_ids[len(data["input_ids"][0]):]
    output = tokenizer.decode(output_ids, skip_special_tokens=True)
    return output.strip()

config = PeftConfig.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(
    config.base_model_name_or_path,
    load_in_8bit=True,
    torch_dtype=torch.float16,
    device_map="auto"
)
model = PeftModel.from_pretrained(
    model,
    MODEL_NAME,
    torch_dtype=torch.float16
)
model.eval()

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)
generation_config = GenerationConfig.from_pretrained(MODEL_NAME)
print(generation_config)

conversation = Conversation()
while True:
    user_message = input("User: ")

    if user_message.strip() == "/reset":
        conversation = Conversation()
        print("History reset completed!")
        continue

    if user_message.strip() == "":
        continue

    conversation.add_user_message(user_message)
    prompt = conversation.get_prompt(tokenizer)
    output = generate(
        model=model,
        tokenizer=tokenizer,
        prompt=prompt,
        generation_config=generation_config
    )
    conversation.add_bot_message(output)
    print("ruGPT-3.5:", output)
    print()
    print("==============================")
    print()
```

## Examples

```text
User: –ü—Ä–∏–≤–µ—Ç! –ü–æ—á–µ–º—É —Ç—Ä–∞–≤–∞ –∑–µ–ª—ë–Ω–∞—è?
ruGPT-3.5: –¢—Ä–∞–≤–∞ –∑–µ–ª—ë–Ω–∞—è –±–ª–∞–≥–æ–¥–∞—Ä—è —Ö–ª–æ—Ä–æ—Ñ–∏–ª–ª—É - –ø–∏–≥–º–µ–Ω—Ç—É —Ä–∞—Å—Ç–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–≥–ª–æ—â–∞–µ—Ç —Å–≤–µ—Ç–æ–≤—ã–µ –≤–æ–ª–Ω—ã –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –¥–ª–∏–Ω—ã –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∏—Ö –≤ —ç–Ω–µ—Ä–≥–∏—é —Ö–∏–º–∏—á–µ—Å–∫–∏—Ö —Å–≤—è–∑–µ–π. –•–ª–æ—Ä–æ—Ñ–∏–ª–ª —Ç–∞–∫–∂–µ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Ä–∞—Å—Ç–µ–Ω–∏—è –ø–∏—Ç–∞—Ç–µ–ª—å–Ω—ã–º–∏ –≤–µ—â–µ—Å—Ç–≤–∞–º–∏ –∏–∑ –ø–æ—á–≤—ã —á–µ—Ä–µ–∑ —Ñ–æ—Ç–æ—Å–∏–Ω—Ç–µ–∑.
```

```text
User: –ü—Ä–∏–¥—É–º–∞–π —Å—Ç–∏—à–æ–∫ –ø—Ä–æ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏.
ruGPT-3.5: –ù–µ–π—Ä–æ—Å–µ—Ç—å ‚Äì —ç—Ç–æ —á—É–¥–æ –Ω–∞—É–∫–∏,
–û–Ω–∞ —Å–ø–æ—Å–æ–±–Ω–∞ —Ä–µ—à–∞—Ç—å –ª—é–±—ã–µ –∑–∞–¥–∞—á–∏.
–ò –µ—Å–ª–∏ —Ç—ã —Ö–æ—á–µ—à—å —Å—Ç–∞—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–æ–º,
–¢–æ –ª—É—á—à–µ –≤—Å–µ–≥–æ –∏–∑—É—á–∞—Ç—å –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏.
```

```text
User: –í—ã—Å–æ—Ç–∞ —Å–∞–º–æ–π –≤—ã—Å–æ–∫–æ–π –≥–æ—Ä—ã –Ω–∞ –ó–µ–º–ª–µ?
ruGPT-3.5: –°–∞–º–∞—è –≤—ã—Å–æ–∫–∞—è –≥–æ—Ä–∞ –Ω–∞ –ó–µ–º–ª–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –ì–∏–º–∞–ª–∞—è—Ö –∏ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è –≠–≤–µ—Ä–µ—Å—Ç (8848 –º–µ—Ç—Ä–æ–≤).
```

```text
User: –°–∫–æ–ª—å–∫–æ –º–∞–∫—Å–∏–º—É–º –ª–µ—Ç –º–æ–∂–µ—Ç –ø—Ä–æ–∂–∏—Ç—å –≥–∞–ª–æ–ø–æ–≥–æ—Å—Å–∫–∞—è —á–µ—Ä–µ–ø–∞—Ö–∞?
ruGPT-3.5: –ì–∞–ª–æ–ø–æ–≥–æ—Å—Å–∫–∏–µ —á–µ—Ä–µ–ø–∞—Ö–∏ –º–æ–≥—É—Ç –∂–∏—Ç—å –¥–æ 100 –ª–µ—Ç –∏ –±–æ–ª–µ–µ. –û–Ω–∏ —è–≤–ª—è—é—Ç—Å—è –æ–¥–Ω–∏–º–∏ –∏–∑ —Å–∞–º—ã—Ö –¥–æ–ª–≥–æ–∂–∏–≤—É—â–∏—Ö –∂–∏–≤–æ—Ç–Ω—ã—Ö –Ω–∞ –ø–ª–∞–Ω–µ—Ç–µ.
```

## üìö Training Datasets

The datasets utilized for training this model are consistent with those used for [Saiga-2](https://github.com/IlyaGusev/rulm).

Here's the comprehensive list:

- [ru_turbo_alpaca](https://huggingface.co/datasets/IlyaGusev/ru_turbo_alpaca)
- [ru_turbo_alpaca_evol_instruct](https://huggingface.co/datasets/IlyaGusev/ru_turbo_alpaca_evol_instruct)
- [ru_turbo_saiga](https://huggingface.co/datasets/IlyaGusev/ru_turbo_saiga)
- [ru_sharegpt_cleaned](https://huggingface.co/datasets/IlyaGusev/ru_sharegpt_cleaned)
- [oasst1_ru_main_branch](https://huggingface.co/datasets/IlyaGusev/oasst1_ru_main_branch)
- [gpt_roleplay_realm](https://huggingface.co/datasets/IlyaGusev/gpt_roleplay_realm)
- [ru_instruct_gpt4](https://huggingface.co/datasets/lksy/ru_instruct_gpt4)

## üõ† Training Procedure

The following `bitsandbytes` quantization config was used during training:

- quant_method: bitsandbytes
- load_in_8bit: True
- load_in_4bit: False
- llm_int8_threshold: 6.0
- llm_int8_skip_modules: None
- llm_int8_enable_fp32_cpu_offload: False
- llm_int8_has_fp16_weight: False
- bnb_4bit_quant_type: fp4
- bnb_4bit_use_double_quant: False
- bnb_4bit_compute_dtype: float32

## ‚öôÔ∏è Framework Versions

Ensure you have the following framework versions for compatibility:

- PyTorch 2.1.0
- PEFT 0.5.0
- bitsandbytes 0.41.1
- transformers 4.34.0

## Links

- https://t.me/evilfreelancer
- https://dzen.ru/evilfreelancer
