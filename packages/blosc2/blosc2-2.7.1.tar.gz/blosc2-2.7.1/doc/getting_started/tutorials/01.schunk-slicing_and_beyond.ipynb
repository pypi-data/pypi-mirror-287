{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slicing, extending and serializing\n",
    "\n",
    "The newest and coolest way to store data in python-blosc2 is through a `SChunk` (super-chunk) object. Here the data is split into chunks of the same size. In the past, the only way of working with it was chunk by chunk (see [the SChunk basics tutorial](00.schunk-basics.html)), but now, python-blosc2 can retrieve, update or append data at item level (i.e. avoiding doing it chunk by chunk). To see how this works, let's first create our SChunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T10:20:35.507212Z",
     "start_time": "2023-06-20T10:20:35.228091Z"
    }
   },
   "outputs": [],
   "source": [
    "import blosc2\n",
    "import numpy as np\n",
    "\n",
    "nchunks = 10\n",
    "data = np.arange(200 * 1000 * nchunks, dtype=np.int32)\n",
    "cparams = {\"typesize\": 4}\n",
    "schunk = blosc2.SChunk(chunksize=200 * 1000 * 4, data=data, cparams=cparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to set the `typesize` correctly as these methods will work with items and not with bytes.\n",
    "\n",
    "## Getting data from a SChunk\n",
    "\n",
    "Let's begin by retrieving the data from the whole SChunk. We could use the `decompress_chunk` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T10:20:35.511148Z",
     "start_time": "2023-06-20T10:20:35.270231Z"
    }
   },
   "outputs": [],
   "source": [
    "out = np.empty(200 * 1000 * nchunks, dtype=np.int32)\n",
    "for i in range(nchunks):\n",
    "    schunk.decompress_chunk(i, out[200 * 1000 * i : 200 * 1000 * (i + 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But instead of the code above, we can simply use the `__getitem__` or the `get_slice` methods. Let's begin with `__getitem__`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T10:20:35.519990Z",
     "start_time": "2023-06-20T10:20:35.304350Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "bytes"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_slice = schunk[:]\n",
    "type(out_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the data is returned as a bytes object. If we want to get a more meaningful container instead, we can use `get_slice`, where you can pass any Python object (supporting the Buffer Protocol) as the `out` param to fill it with the data.  In this case we will use a NumPy array container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T10:20:35.525236Z",
     "start_time": "2023-06-20T10:20:35.336814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "out_slice = np.empty(200 * 1000 * nchunks, dtype=np.int32)\n",
    "schunk.get_slice(out=out_slice)\n",
    "np.array_equal(out, out_slice)\n",
    "print(out_slice[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the expected data indeed!\n",
    "\n",
    "## Setting data in a SChunk\n",
    "\n",
    "We can also set the data of a `SChunk` area from any Python object supporting the Buffer Protocol. Let's see a quick example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T10:20:35.527561Z",
     "start_time": "2023-06-20T10:20:35.353345Z"
    }
   },
   "outputs": [],
   "source": [
    "start = 34\n",
    "stop = 1000 * 200 * 4\n",
    "new_value = np.ones(stop - start, dtype=np.int32)\n",
    "schunk[start:stop] = new_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen how to get or set data. But what if we would like to add data? Well, you can still do that with `__setitem__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T10:20:35.527871Z",
     "start_time": "2023-06-20T10:20:35.368583Z"
    }
   },
   "outputs": [],
   "source": [
    "schunk_nelems = 1000 * 200 * nchunks\n",
    "\n",
    "new_value = np.zeros(1000 * 200 * 2 + 53, dtype=np.int32)\n",
    "start = schunk_nelems - 123\n",
    "new_nitems = start + new_value.size\n",
    "schunk[start:new_nitems] = new_value"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, `start` is less than the number of elements in `SChunk` and `new_items` is larger than this; that means that `__setitem__` can update and append data at the same time, and you don't have to worry about whether you are exceeding the limits of the `SChunk`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a SChunk from/as a contiguous buffer\n",
    "\n",
    "Furthermore, you can convert a SChunk to a contiguous, serialized buffer and vice-versa. Let's get that buffer (aka `cframe`) first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T10:20:35.529947Z",
     "start_time": "2023-06-20T10:20:35.379228Z"
    }
   },
   "outputs": [],
   "source": [
    "buf = schunk.to_cframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the other way around:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T10:20:35.542962Z",
     "start_time": "2023-06-20T10:20:35.388249Z"
    }
   },
   "outputs": [],
   "source": [
    "schunk2 = blosc2.schunk_from_cframe(cframe=buf, copy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we set the `copy` param to `True`. If you do not want to copy the buffer, be mindful that you will have to keep a reference to it until you do not want the SChunk anymore.\n",
    "\n",
    "\n",
    "## Serializing NumPy arrays\n",
    "\n",
    "If what you want is to create a serialized, compressed version of a NumPy array, you can use the newer (and faster) functions to store it either in-memory or on-disk.  The specification of such a contiguous compressed representation, aka **cframe** can be seen [here](https://github.com/Blosc/c-blosc2/blob/main/README_CFRAME_FORMAT.rst).\n",
    "\n",
    "### In-memory\n",
    "\n",
    "For obtaining an in-memory representation, you can use `pack_tensor`. In comparison with its former version (`pack_array`), it is way faster and does not have the 2 GB size limitation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T10:20:54.023391Z",
     "start_time": "2023-06-20T10:20:35.396904Z"
    }
   },
   "outputs": [],
   "source": [
    "np_array = np.arange(2**30, dtype=np.int32)  # 4 GB array\n",
    "\n",
    "packed_arr2 = blosc2.pack_tensor(np_array)\n",
    "unpacked_arr2 = blosc2.unpack_tensor(packed_arr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On-disk\n",
    "\n",
    "To store the serialized buffer on-disk you want to use `save_tensor` and `load_tensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T10:21:15.080447Z",
     "start_time": "2023-06-20T10:20:54.892944Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blosc2.save_tensor(np_array, urlpath=\"ondisk_array.b2frame\", mode=\"w\")\n",
    "np_array2 = blosc2.load_tensor(\"ondisk_array.b2frame\")\n",
    "np.array_equal(np_array, np_array2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Now python-blosc2 offers an easy, yet fast way of creating, getting, setting and expanding data via the `SChunk` class.  Moreover, you can get a contiguous compressed representation (aka [cframe](https://github.com/Blosc/c-blosc2/blob/main/README_CFRAME_FORMAT.rst)) of it and re-create it again later with no sweat.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
