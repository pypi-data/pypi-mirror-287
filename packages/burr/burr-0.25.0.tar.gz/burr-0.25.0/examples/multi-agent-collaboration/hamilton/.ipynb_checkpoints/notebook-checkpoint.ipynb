{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a80d8df22739b896",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Basic Multi-agent Collaboration\n",
    "\n",
    "A single agent can usually operate effectively using a handful of tools within a single domain, but even using powerful models like `gpt-4`, it can be less effective at using many tools. \n",
    "\n",
    "One way to approach complicated tasks is through a \"divide-and-conquer\" approach: create a \"specialized agent\" for each task or domain and route tasks to the correct \"expert\". This means that each agent can become a sequence of LLM calls that chooses how to use a specific \"tool\".\n",
    "\n",
    "This notebook (inspired by the paper [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation](https://arxiv.org/abs/2308.08155), by Wu, et. al.) shows one way to do this using Burr.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41629c14988dec58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T22:01:33.103630Z",
     "start_time": "2024-04-14T22:01:33.100150Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# %pip install -U burr[start] langchain-community langchain-core langchain-experimental openai sf-hamilton[visualization]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "642649bc6414efb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T22:03:15.394918Z",
     "start_time": "2024-04-14T22:03:15.392234Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Environment variables\n",
    "import os\n",
    "# Make sure TAVILY_API_KEY & OPENAI_API_KEY are set\n",
    "# os.environ['TAVILY_API_KEY'] = 'your_tavily_api_key' # get one at https://tavily.com\n",
    "# os.environ['OPENAI_API_KEY'] = 'your_openai_api_key' # get one at https://platform.openai.com"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bd6ddb8fb909d38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T22:14:30.125534Z",
     "start_time": "2024-04-14T22:14:27.831337Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# import everything that you'll need\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "from hamilton import driver\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "from burr import core\n",
    "from burr.core import ApplicationBuilder, State, action, default\n",
    "from burr.lifecycle import PostRunStepHook\n",
    "from burr.tracking import client as burr_tclient\n",
    "\n",
    "# our hamilton module -- see below\n",
    "import func_agent"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1ca3c55452268be9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    " # Define the tools that the agents will use\n",
    "\n",
    "Here we construct the python objects that will be used as tools by our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed8e5eefad5726d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T22:14:30.132311Z",
     "start_time": "2024-04-14T22:14:30.127372Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "repl = PythonREPL()\n",
    "tavily_tool = TavilySearchResults(max_results=5)\n",
    "\n",
    "def python_repl(code: str) -> dict:\n",
    "    \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\n",
    "\n",
    "    :param code: string. The python code to execute.\n",
    "    :return: the output\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return {\"error\": repr(e), \"status\": \"error\", \"code\": f\"```python\\n{code}\\n```\"}\n",
    "    return {\"status\": \"success\", \"code\": f\"```python\\n{code}\\n```\", \"Stdout\": result}\n",
    "\n",
    "# These are our tools that we will use in the application.\n",
    "tools = [tavily_tool, python_repl]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6126c2b461163f05",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Define the agents\n",
    "Our \"agents\" are effectively an execution of a series of LLM calls. \n",
    "In this example we use Hamilton to orchestrate this series of LLM calls.\n",
    "\n",
    "For the source code and to see all the prompts used, see [func_agent.py](./func_agent.py). You'll see the structure of\n",
    "this as the output of the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c37033a4a91bd2f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T22:14:30.791938Z",
     "start_time": "2024-04-14T22:14:30.358694Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# The Agent that we'll use. Our agents here only differ by the system message passed in.\n",
    "agent_dag = driver.Builder().with_modules(func_agent).build()\n",
    "agent_dag"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e0ce269d3e58716b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Define the actions that map to agents\n",
    "We now then create specific actions that map to the agents we need for this not example.\n",
    "    \n",
    "We want a \"chart generator\" action, that will map to an agent that can generate a chart based on provided context/data.\n",
    "\n",
    "We want a \"researcher\" action, that will map to an agent that can search for information on a topic.\n",
    "\n",
    "We then want a \"tool_node\" action, that will run a tool as specified by the prior action, i.e. agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d262371331b8f996",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T22:28:18.243782Z",
     "start_time": "2024-04-14T22:28:18.231891Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "@action(reads=[\"query\", \"messages\"], writes=[\"messages\"])\n",
    "def chart_generator(state: State) -> tuple[dict, State]:\n",
    "    \"\"\"The chart generator action.\n",
    "\n",
    "    :param state: state of the application\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    query = state[\"query\"]\n",
    "    result = agent_dag.execute(\n",
    "        [\"parsed_tool_calls\", \"llm_function_message\"],\n",
    "        inputs={\n",
    "            \"tools\": [python_repl],\n",
    "            \"system_message\": \"Any charts you display will be visible by the user. When done say 'FINAL ANSWER'.\",\n",
    "            \"user_query\": query,\n",
    "            \"messages\": state[\"messages\"],\n",
    "        },\n",
    "    )\n",
    "    new_message = result[\"llm_function_message\"]\n",
    "    parsed_tool_calls = result[\"parsed_tool_calls\"]\n",
    "    state = state.update(parsed_tool_calls=parsed_tool_calls)\n",
    "    state = state.append(messages=new_message)\n",
    "    state = state.update(sender=\"chart_generator\")\n",
    "    return result, state\n",
    "\n",
    "@action(reads=[\"query\", \"messages\"], writes=[\"messages\"])\n",
    "def researcher(state: State) -> tuple[dict, State]:\n",
    "    \"\"\"The researcher action.\n",
    "\n",
    "    :param state: state of the application\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    query = state[\"query\"]\n",
    "    result = agent_dag.execute(\n",
    "        [\"parsed_tool_calls\", \"llm_function_message\"],\n",
    "        inputs={\n",
    "            \"tools\": [tavily_tool],\n",
    "            \"system_message\": \"You should provide accurate data for the chart generator to use. When done say 'FINAL ANSWER'.\",\n",
    "            \"user_query\": query,\n",
    "            \"messages\": state[\"messages\"],\n",
    "        },\n",
    "    )\n",
    "    new_message = result[\"llm_function_message\"]\n",
    "    parsed_tool_calls = result[\"parsed_tool_calls\"]\n",
    "    state = state.update(parsed_tool_calls=parsed_tool_calls)\n",
    "    state = state.append(messages=new_message)\n",
    "    state = state.update(sender=\"researcher\")\n",
    "    return result, state\n",
    "\n",
    "\n",
    "@action(reads=[\"messages\", \"parsed_tool_calls\"], writes=[\"messages\", \"parsed_tool_calls\"])\n",
    "def tool_node(state: State) -> tuple[dict, State]:\n",
    "    \"\"\"Given a tool call, execute it and return the result.\"\"\"\n",
    "    new_messages = []\n",
    "    parsed_tool_calls = state[\"parsed_tool_calls\"]\n",
    "\n",
    "    for tool_call in parsed_tool_calls:\n",
    "        tool_name = tool_call[\"function_name\"]\n",
    "        tool_args = tool_call[\"function_args\"]\n",
    "        tool_found = False\n",
    "        for tool in tools:\n",
    "            name = getattr(tool, \"name\", None)\n",
    "            if name is None:\n",
    "                name = tool.__name__\n",
    "            if name == tool_name:\n",
    "                tool_found = True\n",
    "                kwargs = json.loads(tool_args)\n",
    "                # Execute the tool!\n",
    "                if hasattr(tool, \"_run\"):\n",
    "                    result = tool._run(**kwargs)\n",
    "                else:\n",
    "                    result = tool(**kwargs)\n",
    "                new_messages.append(\n",
    "                    {\n",
    "                        \"tool_call_id\": tool_call[\"id\"],\n",
    "                        \"role\": \"tool\",\n",
    "                        \"name\": tool_name,\n",
    "                        \"content\": result,\n",
    "                    }\n",
    "                )\n",
    "        if not tool_found:\n",
    "            raise ValueError(f\"Tool {tool_name} not found.\")\n",
    "\n",
    "    for tool_result in new_messages:\n",
    "        state = state.append(messages=tool_result)\n",
    "    state = state.update(parsed_tool_calls=[])\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": new_messages}, state\n",
    "\n",
    "@action(reads=[], writes=[])\n",
    "def terminal_step(state: State) -> tuple[dict, State]:\n",
    "    \"\"\"Terminal step we have here that does nothing, but it could\"\"\"\n",
    "    return {}, state"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2f467fb7ae6360f9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Define the Graph / Application\n",
    "With Burr we need to now construct our application, i.e. graph, by:\n",
    "\n",
    "1. Defining what the actions are and how to transition between them.\n",
    "2. Defining the initial state of the application. In our example this means we need to provide a \"query\" for the agents to work on.\n",
    "\n",
    "Because Burr comes with built in persistence, we can also load a prior execution and continue from \n",
    "any point in its history by specifying a `app_instance_id` and `sequence_number` when building the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79b4924965e77be3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T22:27:20.323166Z",
     "start_time": "2024-04-14T22:27:20.319966Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Adjust these if you want to load a prior execution\n",
    "app_instance_id = None\n",
    "sequence_id = None\n",
    "project_name = \"demo:hamilton-multi-agent-v1\"\n",
    "\n",
    "# CHANGE THIS IF YOU WANT SOMETHING DIFFERENT!\n",
    "default_query = (\"Fetch the UK's GDP over the past 5 years, then draw a line graph of it. \"\n",
    "                 \"Once the python code has been written and the graph drawn, the task is complete.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8d69620528ea842",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T22:27:24.193962Z",
     "start_time": "2024-04-14T22:27:24.188457Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Determine initial state and entry point\n",
    "def default_state_and_entry_point() -> tuple[dict, str]:\n",
    "    \"\"\"Returns the default state and entry point for the application.\"\"\"\n",
    "    return {\n",
    "        \"messages\": [],\n",
    "        \"query\": default_query,\n",
    "        \"sender\": \"\",\n",
    "        \"parsed_tool_calls\": [],\n",
    "    }, \"researcher\"\n",
    "\n",
    "if app_instance_id:\n",
    "    tracker = burr_tclient.LocalTrackingClient(project_name)\n",
    "    persisted_state = tracker.load(\"demo\", app_id=app_instance_id, sequence_no=sequence_id)\n",
    "    if not persisted_state:\n",
    "        print(f\"Warning: No persisted state found for app_id {app_instance_id}.\")\n",
    "        state, entry_point = default_state_and_entry_point()\n",
    "    else:\n",
    "        state = persisted_state[\"state\"]\n",
    "        entry_point = persisted_state[\"position\"]\n",
    "else:\n",
    "    state, entry_point = default_state_and_entry_point()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5812de2d0cdc8f21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T22:28:23.019998Z",
     "start_time": "2024-04-14T22:28:22.190999Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Build the application \n",
    "def build_application(state: dict, entry_point: str):\n",
    "    _app = (\n",
    "        ApplicationBuilder()\n",
    "        # set the actions\n",
    "        .with_actions(\n",
    "            researcher=researcher,\n",
    "            chart_generator=chart_generator,\n",
    "            tool_node=tool_node,\n",
    "            terminal=terminal_step,\n",
    "        )\n",
    "        # set the transitions\n",
    "        .with_transitions(\n",
    "            (\"researcher\", \"tool_node\", core.expr(\"len(parsed_tool_calls) > 0\")),\n",
    "            (\n",
    "                \"researcher\",\n",
    "                \"terminal\",\n",
    "                core.expr(\"'FINAL ANSWER' in messages[-1]['content']\"),\n",
    "            ),\n",
    "            (\"researcher\", \"chart_generator\", default),\n",
    "            (\"chart_generator\", \"tool_node\", core.expr(\"len(parsed_tool_calls) > 0\")),\n",
    "            (\n",
    "                \"chart_generator\",\n",
    "                \"terminal\",\n",
    "                core.expr(\"'FINAL ANSWER' in messages[-1]['content']\"),\n",
    "            ),\n",
    "            (\"chart_generator\", \"researcher\", default),\n",
    "            (\"tool_node\", \"researcher\", core.expr(\"sender == 'researcher'\")),\n",
    "            (\"tool_node\", \"chart_generator\", core.expr(\"sender == 'chart_generator'\")),\n",
    "        )\n",
    "        # set a few other things\n",
    "        .with_identifiers(partition_key=\"demo\")\n",
    "        .with_state(**state)\n",
    "        .with_entrypoint(entry_point)\n",
    "        .with_tracker(project=project_name)\n",
    "        .build()\n",
    "    )\n",
    "    return _app\n",
    "app = build_application(state, entry_point)\n",
    "app.visualize(\n",
    "    output_file_path=\"statemachine\", include_conditions=True, format=\"png\"\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b05e65c77dcc38b5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# open up the Burr UI to trace the execution\n",
    "In another terminal run:\n",
    "```bash\n",
    "burr\n",
    "```\n",
    "and then open up the browser to [http://localhost:7241](http://localhost:7241) to see the execution of the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7173415dbbb0b554",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T22:29:47.695377Z",
     "start_time": "2024-04-14T22:28:41.193974Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# this will run until completion.\n",
    "last_action, last_result, last_state = app.run(halt_after=[\"terminal\"])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2880bdfee354b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T22:30:48.865592Z",
     "start_time": "2024-04-14T22:30:48.861637Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "source": [
    "pprint.pprint(last_state)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dedde58d-f684-49d9-81a3-df52e75c447d",
   "metadata": {},
   "source": [
    "# Change the Query!\n",
    "Right now we provide the starting query as state. So we just create a new application by adjusting \n",
    "the initial state we provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8883f456-2217-4802-be0f-3e14cc1475bd",
   "metadata": {},
   "source": [
    "# Let's change the query\n",
    "state[\"query\"] = (\"Fetch the USA's GDP over the past 5 years, then draw a line graph of it. \"\n",
    "                 \"Once the python code has been written and the graph drawn, the task is complete.\")\n",
    "app2 = build_application(state, entry_point)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d0b8a14d4614069",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T22:30:27.699594Z",
     "start_time": "2024-04-14T22:30:27.696685Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# this will run until completion.\n",
    "last_action, last_result, last_state = app2.run(halt_after=[\"terminal\"])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54543e628379f27a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "pprint.pprint(last_state)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e15b805-4feb-4541-8d2f-5075368bb293",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
