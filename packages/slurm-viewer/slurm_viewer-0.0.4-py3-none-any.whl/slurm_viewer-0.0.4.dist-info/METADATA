Metadata-Version: 2.1
Name: slurm-viewer
Version: 0.0.4
Summary: View a SLURM cluster and inspect nodes and jobs.
Author-email: Patrick de Koning <pjhdekoning@lumc.nl>
Project-URL: Homepage, https://gitlab.com/lkeb/slurm_viewer
Keywords: XNAT,TUI
Classifier: Operating System :: OS Independent
Classifier: Development Status :: 2 - Pre-Alpha
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Healthcare Industry
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Natural Language :: English
Classifier: Programming Language :: Python :: 3
Classifier: Environment :: Console
Classifier: Topic :: Scientific/Engineering :: Bio-Informatics
Classifier: Topic :: Scientific/Engineering :: Medical Science Apps.
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: pydantic
Requires-Dist: python-dateutil
Requires-Dist: textual >=0.73.0
Requires-Dist: textual-plotext
Requires-Dist: tomlkit
Requires-Dist: asyncssh
Requires-Dist: eval-type-backport ; python_version < "3.10"

# Slurm Viewer

## Introduction

View the status of a SLURM cluster, including nodes and queue. This application can be run on the cluster itself or any
computer that can ssh into the cluster. Using it via a ssh connection, especially using a jump host can be slow.

Features:
- Overview of all nodes or just nodes in a set of partitions.
- Limit to nodes with GPUs / available GPUs.
- Show the running jobs on a selection of partitions and the jobs waiting to be scheduled.
- Show the GPU memory used over the last 4 weeks.

View the nodes in the selected partitions.
![Slurmviewer Nodes](https://gitlab.com/lkeb/slurm_viewer/-/raw/main/slurmviewer_nodes.svg "Nodes")
View the queue of running and pending jobs.
![Slurmviewer Queue](https://gitlab.com/lkeb/slurm_viewer/-/raw/main/slurmviewer_queue.svg "Queue")
View the GPU utilization and memory usage
![Slurmviewer SPU](https://gitlab.com/lkeb/slurm_viewer/-/raw/main/slurmviewer_gpu.svg "GPU")

## Installation

```bash
pip install slurm-viewer
```

## Usage

Run `slurm-viewer-init` to create a default settings file stored in `~/.config/slurm-viewer/settings.toml`.
Edit this to reflect your setup. Once you have finished run `slurm-viewer` to start the UI.

## Settings

The config files consist of several sections. You can add multiple slurm clusters.

```toml
[ui]
node_columns = ["node_name", "state", "gpu_tot", "gpu_alloc", "gpu_avail", "gpu_type", "gpu_mem", "cpu_tot", "cpu_alloc", "cpu_avail", "mem_tot", "mem_alloc", "mem_avail", "cpu_gpu", "mem_gpu", "cpuload", "partitions", "active_features"]
queue_columns = ["user", "job_id", "reason", "exec_host", "start_delay", "run_time", "time_limit", "command"]
priority_columns = ["user_name", "job_id", "job_priority_n", "age_n", "fair_share_n", "partition_name"]

[[clusters]]
name = "cluster_1"
partitions = ["cpu", "gpu"]
node_name_ignore_prefix = ["node"]
server = "cluster_1_logon_node"

[[clusters]]
name = "cluster_2"
partitions = ["cpu-short", "cpu-medium", "cpu-long", "gpu-short", "gpu-medium", "gpu-long"]
server = "cluster_2.logon.node"

```

If you need to connect using a jumphost/gateway use the `~/.ssh/config` to setup the connections and use the `Host` name as
the server.

Example of a ssh config:

```
Host gateway_1
  User my_user_name
  HostName gateway.somewhere
  
Host cluster_1
  User my_user_name
  HostName logonnode.somewhere
  ProxyCommand ssh -W %h:%p gateway_1
```
