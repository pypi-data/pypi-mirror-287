__all__ = (
    'RecursiveDescentParser',
    'FileParser',
    'Unparser',
    'PyParser',
)


from ast import (
    AST,
    Assign,
    Attribute,
    Constant,
    Dict,
    List,
    Module,
    Name,
    NodeVisitor,
    UnaryOp,
)
from os import PathLike
from collections.abc import Callable, Iterable, Mapping, Sequence
from types import GeneratorType
from typing import Any, NoReturn

from .error import NotAMappingError, ParseError
from .compiler import Compiler
from .lexer import Lexer
from .token import(
    CharNo,
    ColNo,
    LineNo,
    Token,
    TokType,
    TokGroup as TG,
)


type FileContents = str
type PythonData = str


# FIRST() functions
type FIRST = set
MatchesDotted: FIRST = {TokType.NAME}
MatchesMaybeEQ: FIRST = {TokType.EQUAL, None}
MatchesExpr: FIRST = {
    *TG.T_Literal,
    TokType.LSQB,
    TokType.LBRACE,
    TokType.NEWLINE,
    TokType.COMMA,
    None
}
MatchesList: FIRST = {TokType.LSQB}
MatchesRepeatedExpr: FIRST = {*MatchesExpr, None}
MatchesDict: FIRST = {TokType.LBRACE}
MatchesRepeatedKVP: FIRST = {TokType.NAME, None}
MatchesKVPair: FIRST = {TokType.NAME}
MatchesDelimiter: FIRST = {TokType.NEWLINE, TokType.COMMA, None}
MatchesUnaryOp: FIRST = {
    TokType.MINUS,
    TokType.PLUS,
    TokType.EXCLAMATION,
    TokType.TILDE
}

# Return types
type Expr = Constant | List | Dict | Name
type Identifier = Name | Attribute
type Delimiter = None
type Dotted = tuple[Name, bool]
type MaybeEQ = None
type RepeatedExpr = list[Expr]
type KVPair = tuple[Identifier, Expr]
type RepeatedKVP = list[KVPair]


class RecursiveDescentParser:
    '''
    Parse strings, converting them to abstract syntax trees.
    Use generators and stacks to overcome Python's recursion limit.

    :param lexer: The lexer used for feeding tokens
    :type lexer: :class:`Lexer`

    :param string: A string to parse if `lexer` is not given
    :type string: :class:`FileContents`
    '''
    __slots__ = (
        '_cursor',
        '_expr_stack',
        '_lexer',
        '_string',
        '_tokens',
    )

    def __init__(
        self,
        lexer: Lexer = None,
        string: FileContents = None
    ) -> None:
        if lexer is None:
            if string is None:
                msg = (
                    "A `string` argument is required when `lexer` is"
                    " not given."
                )
                raise ValueError(msg)
            lexer = Lexer(string)
        self._lexer = lexer
        self._string = string
        self._tokens = self._lexer.lex()
        self._cursor = 0
        self._expr_stack = []

    End = TokType.ENDMARKER

    @property
    def lookahead(self) -> Token:
        '''
        Return the current token.
        :rtype: :class:`Token`
        '''
        return self._tokens[self._cursor]

    @property
    def tokens(self) -> list[Token]:
        '''
        Return the list of tokens generated by the lexer.
        :rtype: list[:class:`Token`]
        '''
        return self._tokens

    def _token_dict(self) -> dict[CharNo, Token]:
        '''
        Return a dict mapping lexer cursor value to tokens.
        :rtype: dict[:class:`CharNo`, :class:`Token`]
        '''
        return {t.cursor: t for t in self.tokens()}

    def at_eof(self) -> bool:
        '''
        Return whether the parser has reached the end of the token stream.
        :rtype: :class:`bool`
        '''
        return self._cursor == len(self._tokens) - 1

    def _advance(self) -> Token:
        '''
        Move to the next token. Return that token.
        :rtype: :class:`Token`
        '''
        if self._cursor < len(self._tokens) - 1:
            self._cursor += 1
        return self._tokens[self._cursor]

    def _skip_all_whitespace(self) -> Token:
        '''
        Return the current or next non-whitespace token.
        This also skips newlines.
        :rtype: :class:`Token`
        '''
        tok = self._tokens[self._cursor]
        while True:
            if tok.type not in (*TG.T_Ignore, TokType.NEWLINE):
                return tok
            tok = self._advance()

    def _next(self) -> Token:
        '''
        Advance to the next non-whitespace token. Return that token.
        :rtype: :class:`Token`
        '''
        while True:
            if not (tok := self._advance()).type in TG.T_Ignore:
                return tok

    def _match(
        self,
        type_: TokType | Iterable[TokType],
        errmsg: str = None,
        greedy: bool = False
    ) -> Token | NoReturn | bool:
        '''
        Test if the current token matches a certain type given by `type_`.
        If the test fails, raise :exc:`ParseError` using
        `errmsg` or return ``False`` if `errmsg` is ``None``.
        If the test passes, return the current token.

        :param type_: The type(s) to expect from the current token
        :type type_: :class:`TokType`

        :param errmsg: The error message to display, optional
        :type errmsg: :class:`str`

        :param greedy: Consume all matching tokens and return the first
            token that does not match.
        :type greedy: :class:`bool`

        :raises: :exc:`ParseError` When the test fails, `errmsg` is
            given and `greedy` is ``False`` 
        '''
        if not isinstance(type_, Iterable):
            type_ = (type_,)
        tok = self.lookahead
        if greedy:
            while tok.type in type_:
                tok = self._advance()
            return tok
        if tok.type not in type_:
            if errmsg is None:
                return False
            raise ParseError.hl_error(tok, errmsg, self)
        self._advance()
        return tok

    @property
    def prev_tok(self) -> Token:
        '''
        Return the last parsed token.
        '''
        return self._tokens[self._cursor - 1]

    def _reset(self) -> None:
        '''
        Return the lexer to the first token of the token stream.
        '''
        self._cursor = 0
        self._lexer.reset()

    def _parse(self, callback: Callable) -> AST:
        '''
        Use generators and a stack to bypass the Python recursion limit.
        Credit to Dave Beazley (2014).
        Run `callback` outside the Python call stack.

        :param callback: The generator to run
        :type callback: :class:`Callable`
        '''
        stack = [self._gen_parse(callback)]
        result = None
        while stack:
            try:
                callback = stack[-1].send(result)
                stack.append(self._gen_parse(callback))
                result = None
            except StopIteration as e:
                stack.pop()
                result = e.value
        return result

    def _gen_parse(self, callback: Callable) -> AST:
        '''
        Use generators and a stack to bypass the Python recursion limit.
        Credit to Dave Beazley (2014).
        Run `callback` outside the Python call stack by sending it to
        `_parse`.

        :param callback: The generator to send
        :type callback: :class:`Callable`
        '''
        if not callable(callback):
            return callback
        result = callback()
        if isinstance(result, GeneratorType):
            result = (yield from result)
        return result

    def _parse_Assignment(self) -> Assign | None:
        self._skip_all_whitespace()
        if self.at_eof():
            return TokType.ENDMARKER
        target = (yield self._parse_Identifier)
        self._parse_MaybeEQ()
        value = (yield self._parse_factor)
        if value is TokType.NEWLINE:
            value = Constant(None)
        self._parse_Delimiter()
        node = Assign([target], value)
        node._token = target._token
        return node

    def _parse_Identifier(self) -> Name | Attribute:
        base = self.lookahead
        type_ = base.type
        node = Name(base.value)
        node._token = base

        note = ""
        if type_ is not TokType.NAME:
            if type_ in TG.T_Syntax:
                if self._expr_stack:
                    curr_expr = self._expr_stack[-1]
                    suggestion = {
                        Dict: '}',
                        List: ']',
                    }[curr_expr]
                    note = f"Did you mean to use {suggestion!r}? "
                else:
                    note = f"Unmatched {type_.value!r} "

        msg = f"Invalid syntax: {note}(Expected an identifier):"
        self._match(TokType.NAME, msg)

        # This may be the base of another attr, or it may be the
        # terminal attr:
        while self._match(TokType.DOT):
            maybe_base_tok = self._match(TokType.NAME, msg)
            attr = maybe_base_tok.value
            node = Attribute(node, attr)
            node._token = maybe_base_tok
        return node

    def _parse_MaybeEQ(self) -> MaybeEQ:
        self._match(MatchesMaybeEQ)

    def _parse_Delimiter(self) -> Delimiter:
        self._match(MatchesDelimiter, greedy=True)

    def _parse_factor(self):
        tok = self._match(MatchesUnaryOp)
        if tok:
            self._expr_stack.append(UnaryOp)
            type_ = tok.type
            node = UnaryOp(
                type_.value,
                (yield self._parse_factor)
            )
            self._expr_stack.pop()
        else:
            node = (yield self._parse_primary)
        node._token = tok
        return node

    def _parse_primary(self):
        tok = self.lookahead
        type_ = tok.type
        if type_ in TG.T_Literal:
            return self._parse_Constant()
##        # Doesn't work; try something else...
##        if self._expr_stack:
##            if self._expr_stack[-1] is UnaryOp:
##                msg = f"Invalid syntax: Expected a primitive type:"
##                raise ParseError.hl_error(tok, msg, self)
        elif type_ is TokType.NAME:
            node = (yield self._parse_Identifier)
        elif type_ is TokType.LSQB:
            node = (yield self._parse_List)
        elif type_ is TokType.LBRACE:
            node = (yield self._parse_Dict)
        else:
            msg = (f"Expected an expression,"
                   f" but got {tok.value!r} instead:")
            raise ParseError.hl_error(tok, msg, self)
        node._token = tok
        self._skip_all_whitespace()
        return node

    def _parse_Constant(self) -> None:
        tok = self._match(TG.T_Literal)
        if tok:
            type_ = tok.type
            try:
                match type_:
                    case TokType.STRING:
                        value = tok.value
                    case TokType.INTEGER:
                        value = int(tok.value)
                    case TokType.FLOAT:
                        value = float(tok.value)
                    case TokType.TRUE:
                        value = True
                    case TokType.FALSE:
                        value = False
                    case TokType.NONE:
                        value = None
                    case _:
                        msg = f"Expected a literal, but got {tok.value!r}"
                        raise ParseError.hl_error(tok, msg, self)
                node = Constant(value)
            except ValueError:
                # Floats with underscores sometimes fail.
                msg = f"Invalid {type_.value.lower()} value: {tok.value!r}"
                raise ParseError.hl_error(tok, msg, self)
            return node

    def _parse_List(self) -> List:
        self._expr_stack.append(List)
        msg = "_parse_List() called at the wrong time"
        start = self._match(TokType.LSQB, msg)
        elems = (yield self._parse_RepeatedExpr)
        self._match(TokType.RSQB)
        node = List(elems)
        node._token = start
        self._expr_stack.pop()
        return node

    def _parse_RepeatedExpr(self) -> RepeatedExpr:
        start = self.prev_tok
        exprs = []
        while True:
            self._skip_all_whitespace()
            if self.lookahead.type is TokType.ENDMARKER:
                msg = "Invalid syntax: Unmatched '[':"
                raise ParseError.hl_error(start, msg, self)
            if self.lookahead.type is TokType.RSQB:
                break
            expr = (yield self._parse_factor)
            # Match commas and newlines:
            self._parse_Delimiter()
            exprs.append(expr)
        return exprs

    def _parse_Dict(self) -> Dict:
        self._expr_stack.append(Dict)
        msg = "_parse_Dict() called at the wrong time",
        start = self._match(TokType.LBRACE, msg)
        keys, vals = (yield self._parse_RepeatedKVP)
        self._match(TokType.RBRACE)
        node = Dict(keys, vals)
        node._token = start
        self._expr_stack.pop()
        return node

    def _parse_RepeatedKVP(self) -> RepeatedKVP:
        start = self.prev_tok
        keys = []
        vals = []
        while True:
            self._skip_all_whitespace()
            if self.lookahead.type is TokType.ENDMARKER:
                msg = "Invalid syntax: Unmatched '{':"
                raise ParseError.hl_error(start, msg, self)
            if self.lookahead.type is TokType.RBRACE:
                break
            pair = (yield self._parse_KVPair)
            self._parse_Delimiter()
            key, val = pair

            if key not in keys:
                keys.append(key)
                vals.append(val)
            else:
                # Make nested dicts.
                idx = keys.index(key)
                # Equivalent to dict.update():
                vals[idx].keys = key
                vals[idx].values = val
        return keys, vals

    def _parse_KVPair(self) -> KVPair:
        target = (yield self._parse_Identifier)
        self._parse_MaybeEQ()
        value = (yield self._parse_factor)
        # Disallow assigning identifiers:
        if isinstance(value, (Name, Attribute)):
            typ = value._token.type.value.lower()
            val = repr(value._token.value)
            note = f"expected expression, got {typ} {val}"
            msg = f"Invalid assignment: {note}:"
            raise ParseError.hl_error(value._token, msg, self)
        return target, value

    def parse(self, string: FileContents = None) -> Module:
        '''
        Parse the lexer stream and return it as a :class:`Module`.

        :param string: A new string to use for parsing, optional
        :type string: :class:`FileContents`
        '''
        if string is not None:
            self._string = string
            self._lexer = Lexer(string)
            self._tokens = self._lexer.lex()
            self._cursor = 0

        self._reset()
        assignments = []
        while True:
            assign = self._parse(self._parse_Assignment)
            if assign is TokType.ENDMARKER:
                break
            if assign is None:
                # This happens when there are no assignments at all.
                break
            assignments.append(assign)
        self._reset()
        return Module(assignments)

    def to_py(self) -> PythonData:
        '''
        Parse Scuff text and return its equivalent Python data.
        '''
        tree = self.parse()
        unparsed = Compiler().compile(tree)
        return unparsed


class FileParser(RecursiveDescentParser):
    '''
    A class for parsing text files.

    :param file: The filename used to source an input string
    :type file: :class:`PathLike`

    :param lexer: The lexer to use for parsing.
    :type lexer: :class:`Lexer`
    '''
    def __init__(
        self,
        file: PathLike = None,
        *,
        lexer: Lexer = None,
    ) -> None:
        if lexer is None:
            if file is None:
                msg = "Either a `file` or a `lexer` argument is required"
                raise ValueError(msg)
            with open(file, 'r') as f:
                string = f.read()
            lexer = Lexer(string, file)

        self._file = file
        self._string = string
        self._lexer = lexer
        self._tokens = self._lexer.lex()
        self._cursor = 0
        self._expr_stack = []


class Unparser(NodeVisitor):
    '''
    Convert ASTs to string representations using Scuff syntax.
    '''
    def __init__(self) -> None:
        self._indent = 4 * ' '
        self._indent_level = 0

    def indent(self) -> str:
        return self._indent_level * self._indent

    def unparse(
        self,
        tree: Iterable[AST] | Module,
        sep: str = '\n\n'
    ) -> FileContents:
        '''
        Convert an AST to Scuff text.

        :param tree: The ASTs to be converted to strings
        :type tree: :class:`Iterable`[:class:`AST`] | :class:`Module`

        :param sep: The separator to use between tree nodes, defaults to
            ``'\\n\\n'``

        :type sep: :class:`str`
        '''
        if isinstance(tree, Module):
            tree = tree.body
        strings = (self.visit(node) for node in tree)
        return sep.join(strings)

    def visit(self, node: AST):
        '''
        Use a stack and generators to overcome Python's recursion limit.
        Credit to Dave Beazley (2014).
        '''
        stack = [self.genvisit(node)]
        result = None
        while stack:
            try:
                node = stack[-1].send(result)
                stack.append(self.genvisit(node))
                result = None
            except StopIteration as e:
                stack.pop()
                result = e.value
        return result

    def genvisit(self, node: AST):
        '''
        Use generators to overcome Python's recursion limit.
        Credit to Dave Beazley (2014).
        '''
        name = 'visit_' + type(node).__name__
        result = getattr(self, name)(node)
        if isinstance(result, GeneratorType):
            result = (yield from result)
        return result

    def visit_Attribute(self, node: Attribute) -> str:
        base = (yield node.value)
        attr = node.attr
        return f"{base}.{attr}"

    def visit_Assign(self, node: Assign) -> str:
        target = (yield node.targets[-1])
        value = (yield node.value)
        if value == str(None):
            value = ""
        return f"{target} {value}"

    def visit_Constant(self, node: Constant) -> str:
        val = node.value
        if isinstance(val, str):
            return repr(val)
        return str(val)

    def visit_Dict(self, node: Dict) -> str:
        one_line_limit = 1
        opening, closing = "{}"
        joiner = ", "
        multiline = len(node.keys) > one_line_limit
        if multiline:
            self._indent_level += 1
            opening = f"{opening}\n{self.indent()}"
            joiner = f"\n{self.indent()}"
        keys = []
        values = []
        for k in node.keys:
            keys.append((yield k))
        for v in node.values:
            value = (yield v)
            if value == str(None):
                value = ""
            values.append(value)
        assignments = tuple(' '.join(pair) for pair in zip(keys, values))
        if multiline:
            self._indent_level -= 1
            closing = f"\n{self.indent()}{closing}"
        joined = joiner.join(assignments)
        string = f"{opening}{joined}{closing}"
        return string

    def visit_List(self, node: List) -> str:
        one_line_limit = 3
        opening, closing = "[]"
        joiner = ", "
        multiline = len(node.elts) > one_line_limit
        if multiline:
            self._indent_level += 1
            opening = f"{opening}\n{self.indent()}"
            joiner = '\n' + self.indent()
        elems = []
        for e in node.elts:
            elems.append((yield e))
        joined = joiner.join(elems)
        if multiline:
            self._indent_level -= 1
            closing = f"\n{self.indent()}{closing}"
        string = f"{opening}{joined}{closing}"
        return string

    def visit_Name(self, node: Name) -> str:
        string = node.id
        return string

    def visit_UnaryOp(self, node: UnaryOp) -> str:
        return node.op + (yield node.operand)


class PyParser:
    '''
    Convert Python data to ASTs.
    Effectively do the job of `ast.parse()`.

    :param unquoted: Values found at keys in this set and any elements
        within will be represented as symbols (without quotes).
    :type unquoted: :class:`Iterable`
    '''

    def __init__(self, unquoted: Iterable = {}) -> None:
        self.unquoted = unquoted

    def parse(self, data: Mapping) -> Module:
        '''
        Convert a Python mapping to a Module AST.
        Effectively do the job of `ast.unparse()`.

        :param data: The mapping to convert
        :type data: :class:`Mapping`
        '''
        assignments = []
        if not isinstance(data, Mapping):
            raise NotAMappingError(
                f"The outermost data structure must be a mapping."
            )

        for key, val in data.items():
            if key in self.unquoted:
                if isinstance(val, str):
                    val = Name(field)
                elif isinstance(val, Sequence):
                    val = List([Name(field) for field in val])
            else:
                val = self._parse_node(val)
            a = Assign([Name(key)], val)
            assignments.append(a)
        return Module(assignments)

    def to_scuff(self, data: Mapping) -> FileContents:
        '''
        Convert a Python mapping to an AST and return the Scuff text that
        could be parsed back into that AST.

        :param data: The mapping to convert
        :type data: :class:`Mapping`
        '''
        tree = self.parse(data)
        text = Unparser().unparse(tree)
        return text

    def _run_process_nested_dict(
        self,
        dct: dict | Any,
        roots: Sequence[str] = [],
        descended: int = 0
    ) -> tuple[list[list[str]], list[PythonData]]:
        '''
        Unravel nested dicts into keys and assignment values.
        This method runs :meth:`_process_nested_dict()`
        For use with :meth:`_process_nested_attrs()`
        Use generators and a stack to bypass the Python recursion limit.

        :param dct: The nested dict to process, or an inner value of it
        :type dct: :class:`dict` | :class:`Any`

        :param roots: The keys which surround the current `dct`
        :type roots: :class:`Sequence`[:class:`str`], defaults to ``[]``

        :param descended: How far `dct` is from the outermost key
        :type descended: :class:`int`, defaults to ``0``
        '''
        stack = [self._process_nested_dict(dct, roots, descended)]
        result = None
        while stack:
            try:
                args = stack[-1].send(result)
                stack.append(self._process_nested_dict(*args))
                result = None
            except StopIteration as e:
                stack.pop()
                result = e.value
        return result

    @staticmethod
    def _process_nested_dict(
        dct: Mapping | Any,
        roots: Sequence[str] = [],
        descended: int = 0
    ) -> tuple[list[list[str]], list[PythonData]]:
        '''
        Unravel nested dicts into keys and assignment values.
        For use with :meth:`_process_nested_attrs()`

        :param dct: The nested dict to process, or an inner value of it
        :type dct: :class:`dict` | :class:`Any`

        :param roots: The keys which surround the current `dct`
        :type roots: :class:`Sequence`[:class:`str`], defaults to ``[]``

        :param descended: How far `dct` is from the outermost key
        :type descended: :class:`int`, defaults to ``0``

        :returns: Attribute names and assignment values
        '''
        nodes = []
        vals = []
        if not isinstance(dct, dict):
            # An assignment.
            return ([roots], [dct])

        if len(dct) == 1:
            # An attribute.
            for a, v in dct.items():
                roots.append(a)
                result = (yield (v, roots, descended))
                return result

        descended = 0  # Start of a tree.
        for attr, v in dct.items():
            roots.append(attr)
            if isinstance(v, dict):
                if descended < len(roots):
                    descended = -len(roots)
                # Descend into lower tree.
                inner = (yield (v, roots, descended))
                if isinstance(inner, GeneratorType):
                    inner = (yield from inner)
                inner_nodes, inner_vals = inner
                nodes.extend(inner_nodes)
                vals.extend(inner_vals)
            else:
                nodes.append(roots)
                vals.append(v)
            roots = roots[:-descended - 1]  # Reached end of tree.
        return nodes, vals

    @staticmethod
    def _process_nested_attrs(
        attrs: Sequence[Sequence[str]]
    ) -> list[Attribute]:
        '''
        Convert a list of attribute names to an :class:`Attribute` node.

        :param attrs: The list of attribute keynames to convert
        :type attrs: :class:`Sequence`[:class:`Sequence`[:class:`str`]]
        '''
        nodes = []
        if not isinstance(attrs, Sequence):
            return attrs
        for node_attrs in attrs:
            node_attrs = node_attrs[:]
            node = Name(node_attrs.pop(0))
            for attr in node_attrs:
                node = Attribute(node, attr)
            nodes.append(node)
        return nodes

    def _parse_node(self, node: Any) -> AST:
        '''
        Parse a Python object and return its corresponding AST node.

        :param node: The node to parse
        :type node: :class:`Any`
        '''
        if isinstance(node, Mapping):
            keys = []
            vals = []
            attribute = False
            for key, val in node.items():
                if isinstance(val, Mapping):
                    if len(node) == 1:
                        attribute = True

                    # An attribute, possibly nested, or an assignment.
                    attrs, assigns = self._run_process_nested_dict(val, [key])
                    nodes = self._process_nested_attrs(attrs)
                    keys.extend(nodes)
                    vals.extend(assigns)

                else:
                    # An explicit assignment.
                    attribute = False
                    keys.append(Name(key))
                    v = self._parse_node(val)
                    vals.append(v)

            if attribute:
                if len(vals) == 1:
                    return Assign(keys, self._parse_node(*vals))
            return Dict(keys, [self._parse_node(v) for v in vals])

        elif isinstance(node, list):
            values = [self._parse_node(v) for v in node]
            return List(values)

        elif isinstance(node, (int, float)):
            return Constant(node)

        elif isinstance(node, str):
            if node in self.unquoted:
                return Name(node)
            return Constant(node)

        elif node in (None, True, False):
            return Constant(node)

        else:
            return node


